# import requests
# from bs4 import BeautifulSoup
# import re

# def get_headers():
#     return {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#     }

# # ì§€ì €ë¶„í•œ ê³µë°±ê³¼ ì¤„ ë°”ê¿ˆì„ ê¹”ë”í•˜ê²Œ ì •ë¦¬
# def clean_text(text):
#     # 1. ì—°ì†ëœ ì¤„ ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì¤„ì„
#     text = re.sub(r'\n\s*\n', '\n', text)
#     # 2. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
#     text = re.sub(r'\s+', ' ', text)
#     return text.strip()

# # [ì¼ë°˜ ì›¹í˜ì´ì§€ìš©] ê´‘ê³ , ë©”ë‰´, ìŠ¤í¬ë¦½íŠ¸ ê°™ì€ ì“°ë ˆê¸°ë¥¼ ë²„ë¦¬ê³  ë³¸ë¬¸ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
# def extract_general_content(soup):
#     # 1. ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ì²­ì†Œ ë‹¨ê³„)
#     # script: ìë°”ìŠ¤í¬ë¦½íŠ¸ ì½”ë“œ / style: ê¾¸ë¯¸ê¸° ì½”ë“œ / header, footer, nav: ë©”ë‰´ì™€ ë°”ë‹¥ê¸€
#     for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'noscript', 'form']):
#         tag.decompose() # íƒœê·¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.

#     # 2. ë‚¨ì€ ê²ƒ ì¤‘ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#     # body íƒœê·¸ê°€ ìˆìœ¼ë©´ bodyì—ì„œ, ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì¶”ì¶œ
#     target = soup.body if soup.body else soup
#     return clean_text(target.get_text(separator=' '))

# # [ë„¤ì´ë²„ ë¸”ë¡œê·¸ìš©] ìˆ¨ê²¨ì§„ ì§„ì§œ ì£¼ì†Œ(iframe)ë¥¼ ì°¾ì•„ ë‚´ìš©ë§Œ ì¶”ì¶œ
# def extract_naver_blog_content(html_text):
#     # 1. iframe src ì°¾ê¸°
#     match = re.search(r'src="(/PostView\.naver\?.*?)"', html_text)
#     if not match:
#         return None
    
#     # 2. ì§„ì§œ ì£¼ì†Œ ì™„ì„±
#     real_url = f"https://blog.naver.com{match.group(1).replace('&amp;', '&')}"
    
#     # 3. ì§„ì§œ ì£¼ì†Œë¡œ ë‹¤ì‹œ ì ‘ì†
#     response = requests.get(real_url, headers=get_headers())
#     if response.status_code != 200:
#         return None
        
#     soup = BeautifulSoup(response.text, 'html.parser')

#     # 4. ë¸”ë¡œê·¸ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (se-main-container ë˜ëŠ” view)
#     post_div = soup.find('div', class_='se-main-container') or soup.find('div', class_='view')
    
#     if post_div:
#         return clean_text(post_div.get_text(separator=' '))
#     else:
#         # ë³¸ë¬¸ ì˜ì—­ì„ ëª» ì°¾ìœ¼ë©´ ì¼ë°˜ ë°©ì‹ì²˜ëŸ¼ ì „ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
#         return extract_general_content(soup)

# def main_extractor(url):
#     """ëŒ€ì¥ ë¡œë´‡: ì£¼ì†Œë¥¼ ë³´ê³  ì•Œë§ì€ ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤."""
#     try:
#         print(f"ğŸ” ë¶„ì„ ì¤‘: {url}")
#         response = requests.get(url, headers=get_headers(), timeout=10)
        
#         # HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œ
#         if response.status_code != 200:
#             print(f"âŒ Error_Code: {response.status_code}")
#             print(response)
#             return

#         html_text = response.text

#         # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ì§€ í™•ì¸
#         if "blog.naver.com" in url:
#             print("ğŸ’¡ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê°ì§€!")
#             content = extract_naver_blog_content(html_text)
#         else:
#             print("ğŸ’¡ ì¼ë°˜ ì›¹í˜ì´ì§€ ê°ì§€!")
#             soup = BeautifulSoup(html_text, 'html.parser')
#             content = extract_general_content(soup)

#         # ê²°ê³¼ ì¶œë ¥
#         print("-" * 50)
#         if content:
#             print(f"âœ… ì¶”ì¶œ ê²°ê³¼ (ê¸¸ì´: {len(content)}ì):")
#             print(content)
#         else:
#             print("âŒ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#         print("-" * 50)

#     except Exception as e:
#         print(f"ğŸš« ì˜¤ë¥˜ ë°œìƒ: {e}")

# # --- ì‹¤í–‰ ---
# if __name__ == "__main__":
#     # 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ í…ŒìŠ¤íŠ¸
#     target_blog = "https://blog.naver.com/khsbless/224104030727"
#     main_extractor(target_blog)

#     print("\n" + "="*50 + "\n")

#     # 2. ì¼ë°˜ ì›¹í˜ì´ì§€ í…ŒìŠ¤íŠ¸ (ì˜ˆ: í•œêµ­ê²½ì œ ê¸°ì‚¬)
#     target_web = "https://www.hankyung.com/article/202511066930g"
#     main_extractor(target_web)
    
#     # 3. ìœ„í‚¤ë…ìŠ¤ í…ŒìŠ¤íŠ¸(ì‹¤íŒ¨: ë´‡ í¬ë¡¤ë§ ìš°íšŒê¸°ëŠ¥ í•„ìš”)
#     target_web = "https://wikidocs.net/742"
#     main_extractor(target_web)


import requests
import time
import random
from fake_useragent import UserAgent # ê°€ì§œ ì‹ ë¶„ì¦ì„ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬ (pip install fake-useragent)

# 1. ì„¸ì…˜(Session) ë§Œë“¤ê¸°: ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë°©ë¬¸ ê¸°ë¡(ì¿ í‚¤)ì„ ê¸°ì–µí•˜ëŠ” ë„êµ¬
session = requests.Session()

# 2. ì •êµí•œ ê°€ì§œ ì‹ ë¶„ì¦(Header) ë§Œë“¤ê¸°
# (fake_useragentê°€ ì—†ë‹¤ë©´ ì§ì ‘ ê¸´ ë¬¸ìì—´ì„ ë„£ì–´ë„ ë©ë‹ˆë‹¤)
try:
    ua = UserAgent()
    user_agent = ua.random # ë§¤ë²ˆ ë‹¤ë¥¸ ë¸Œë¼ìš°ì €ì¸ ì²™ ë³€ê²½
except:
    # ë§Œì•½ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìœ¼ë©´ ê°€ì¥ ì¼ë°˜ì ì¸ í¬ë¡¬ ë¸Œë¼ìš°ì €ë¡œ ì„¤ì •
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'

headers = {
    'User-Agent': user_agent,
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Referer': 'https://www.google.com/' # "êµ¬ê¸€ ê²€ìƒ‰í•´ì„œ ë“¤ì–´ì™”ì–´ìš”"ë¼ê³  í•‘ê³„ ëŒ€ê¸°
}

# ì„¸ì…˜ì— ì‹ ë¶„ì¦ ë¶€ì°©!
session.headers.update(headers)

def human_request(url):
    """ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ë©° ì ‘ì†í•˜ëŠ” í•¨ìˆ˜"""
    print(f"ğŸ•µï¸ ì ‘ì† ì‹œë„: {url}")
    print(f"ğŸ­ í˜„ì¬ ìœ„ì¥ ì‹ ë¶„: {session.headers['User-Agent'][:30]}...")

    try:
        # 3. ì ‘ì† ì „ ëœë¤í•˜ê²Œ ì‰¬ê¸° (ì‚¬ëŒì¸ ì²™ ì—°ê¸°)
        # 1ì´ˆì—ì„œ 3ì´ˆ ì‚¬ì´ë¡œ ë¬´ì‘ìœ„ë¡œ ì‰½ë‹ˆë‹¤. ë¡œë´‡ì€ ì´ë ‡ê²Œ ì•ˆ ì‰¬ê±°ë“ ìš”.
        sleep_time = random.uniform(1, 3)
        print(f"â˜• {sleep_time:.2f}ì´ˆ ë™ì•ˆ ë”´ì§“í•˜ëŠ” ì¤‘...")
        time.sleep(sleep_time)

        response = session.get(url, timeout=10)
        
        if response.status_code == 200:
            print("âœ… ë¬¸ì§€ê¸° í†µê³¼ ì„±ê³µ!")
            return response.text
        elif response.status_code == 403:
            print("ğŸš« ë¬¸ì§€ê¸°ì—ê²Œ ë“¤ì¼°ìŠµë‹ˆë‹¤! (403 Forbidden)")
        else:
            print(f"âš ï¸ ë¬¸ì œ ë°œìƒ: {response.status_code}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
    
    return None

# --- ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    target_url = "https://wikidocs.net/742"
    html = human_request(target_url)
    
    # (ì—¬ê¸°ì„œ ì•„ê¹Œ ë§Œë“  BeautifulSoup ì½”ë“œë¡œ htmlì„ ë¶„ì„í•˜ë©´ ë©ë‹ˆë‹¤)